\section{はじめに}
ロボティクスにおける視覚言語理解は，ロボットが人間の指示を理解し，環境内のオブジェクトを認識・操作するために重要な技術である．
特に，Vision-Language Model（VLM）の発展により，ロボットは画像情報と自然言語を統合的に処理できるようになった~\cite{brohan2023saycan, brohan2022rt, XIAO2025129963, Ma2024ASO}．
本レポートでは，ロボティクスにおけるマルチモーダル検索，特にシーンテキストを活用した手法に焦点を当て，関連研究を分類・整理する．

\section{研究背景と課題}
実世界環境において，ロボットが自然言語指示に基づいてオブジェクトを検索する際，視覚的に類似したオブジェクトの識別が課題となる．
例えば，類似するオブジェクトが多い環境において「赤いコカコーラの缶を取って」という指示では，色だけでなくラベルのテキスト情報が重要な手がかりとなる．
しかし，従来のマルチモーダル検索手法は，シーンテキスト（ラベル，標識など）を明示的に活用していない~\cite{Long2018SceneTD, Gupta2022}．
本レポートでは，この課題に対する既存研究を以下の4つのカテゴリに分類して整理する．

\section{基盤モデルとロボティクス}
近年，大規模言語モデル（LLM）やVLMなどの基盤モデルをロボティクスに応用する研究が活発化している．
Brohan et al.~\cite{brohan2022rt}は，RT-1を提案し，大規模なロボット軌跡データで訓練されたVision-Language-Actionモデルが，多様な操作タスクにわたって高い汎化性能を示すことを実証した．
さらに，Driess et al.~\cite{Driess2023PaLMEAE}のPaLM-Eは，視覚と言語を統合した大規模モデルをロボット制御に適用し，複雑なタスクの実行を可能にした．
最近では，Black et al.~\cite{Black2025pi05}の$\pi$0，Wen et al.~\cite{wen2025dexvla}のDexVLA，Gemini-Robotics~\cite{team2025gemini}など，より高度な視覚理解とロボット能力を統合したモデルが提案されている．
Li et al.~\cite{li2025controlvla}のControlVLAは，少数ショット学習によるオブジェクト中心の適応を実現している．
これらの手法は優れた性能を示す一方で，シーンテキストを明示的に組み込んでおらず，テキスト情報が重要な意味的グラウンディングを提供するシーンにおいて，マルチモーダル理解が制限されている．

\section{Retrieval-Augmented Generation (RAG) の応用}
RAGは，外部知識を検索して言語生成を強化する手法であり，ロボティクスへの応用が進んでいる．
Xie et al.~\cite{xie2025embodiedrag}のEmbodied-RAGは，一般的な非パラメトリック表現を用いて，Embodiedエージェントのグラウンディングと計画を強化している．
ここで，Embodiedエージェントとは物理的な身体を持たず，テキストやシミュレーション上でのみ行動するエージェントと異なり，現実世界の環境においてセンサーやアクチュエータを通じて知覚・行動を行うエージェントを指す．
同様に，Zhu et al.~\cite{zhu2024raea}，Xu et al.~\cite{xu2024prag}，Monaci et al.~\cite{monaci2025rana}，Wang et al.~\cite{wang2025rag6dpose}も，RAGをEmbodiedエージェントに適用する手法を提案している．
Wang et al.~\cite{wang2025navrag}とFan et al.~\cite{fan2024bevinstructor}は，RAGを使用してナビゲーション指示を生成し，Vision-Languageナビゲーションモデルを改善している．
また，Anwar et al.~\cite{anwar2025remembr}は，RAGベースのメモリを長期的推論に活用する手法を提案し，Kumar et al.~\cite{kumar2025collage}は，デモンストレーションを検索・融合して模倣方策学習を行う手法を提案している．
これらの研究は，検索信号としてシーンテキストを活用しておらず，日常環境において視覚的に類似したオブジェクトを曖昧性解消するという点で改善の余地がある．

\section{マルチモーダル検索手法}
ロボットの自然言語指示に基づくオブジェクト検索は，重要な研究課題である．
Yenamandra et al.~\cite{Yenamandra2023HomeRobotOM}とRoboCup@Home~\cite{robocup}は，自然言語指示に基づくモバイル操作タスクにおける家庭用サービスロボットの性能を評価している．
NLMap~\cite{nlmap}，Sigurdsson et al.~\cite{Sigurdsson2023RRExBoTRR}のRR-ExBoT，DM$^2$RM~\cite{dm2rm}RelaX-Former~\cite{relaxformer}は，環境内のターゲットオブジェクトを識別するためにマルチモーダル検索を実行し，自然言語指示に基づいて関連画像のランク付けリストを出力する．
これらの手法は急速に進歩しているが，本質的に曖昧性解消情報を含むシーンテキスト（ラベル，標識など）の活用には重要なギャップが残っている．

\section{シーンテキスト処理技術}
シーンテキストを含む画像の処理は，多様なタスクで研究されている．
テキスト認識~\cite{Zhao2023MultimodalIL, OTE}，テキスト検出~\cite{Liang_2024_CVPR, STEP, Zheng_2024_CVPR}，テキスト検索~\cite{Zeng2024FocusDA, Zheng_2024_CVPR, vista}，視覚的質問応答~\cite{Biten2021LaTrLT, stvqa, Gao2020MultiModalGN}，テキスト合成~\cite{Cui_2024_CVPR, Duan2024ODMAT, Santoso2023OnMS}など，様々な技術が開発されている．
Mishra et al.~\cite{ocrvqa}は，シーンテキスト検索タスクを導入し，文字を検出・分類してクエリテキストを含む画像の確率を計算する2段階パイプラインを提案した．
Gomez et al.~\cite{GomezMaflaECCV2018single}は，テキスト提案を予測し，クエリテキストとの類似性に基づいて画像をランク付けするエンドツーエンドネットワークを提案した．
ViSTA~\cite{vista}は，BERT~\cite{Devlin2019BERTPO}と線形投影を使用してOCRで抽出されたテキストと位置を埋め込み，トランスフォーマー内の融合トークンを介してテキストと位置を視覚特徴と融合する．
しかし，従来手法はシーンテキストをそのまま使用しており，オブジェクトとその属性との関連性を捉えたナラティブ表現への統合は行われていない．

\section{評価ベンチマーク}
マルチモーダル検索とシーンテキストの評価には，様々なベンチマークが使用されている．
COCO~\cite{Lin2014MicrosoftCC}とFlickr30K~\cite{flickr30k}は，もともと画像キャプショニングベンチマークであるが，マルチモーダル検索タスクに広く使用されている．
REVERIE~\cite{qi2020reverie}とGOAT-Bench~\cite{khanna2024goatbench}は，オブジェクト目標ナビゲーションタスク用に設計されており，LTRRIE~\cite{multirankit}は画像のランク付けによるマルチモーダル検索に焦点を当てている．
ST-VQA~\cite{stvqa}は，シーンテキストベースのVQAに使用され，TextCaps~\cite{textcaps}とCOCO-Text-Captioned~\cite{stacmr}は，シーンテキストを考慮した画像キャプショニングに適している．
Visual Genome~\cite{Krishna2017}には，シーンテキストと詳細なアノテーションを持つ画像が含まれており，RefText~\cite{stan}は，参照表現理解のベンチマークであり，その一部はシーンテキストを活用している．
既存ベンチマークの多くは，シーンテキストを欠いているか，マルチモーダル検索用に設計されていないか，ロボットタスクに適していないか，または過度に単純な指示を含んでいる．
したがって，シーンテキストと視覚コンテンツにわたるきめ細かい推論を必要とする，ロボットのための実世界の指示ベースの検索タスクを評価するには不十分である．

\section{まとめと今後の展望}
本レポートでは，ロボティクスにおけるマルチモーダル検索，特にシーンテキストを活用した手法に関する研究を4つのカテゴリ（基盤モデル，RAG，マルチモーダル検索，シーンテキスト処理）に分類して整理した．
各カテゴリにおいて優れた研究成果が報告されているが，シーンテキストを明示的に活用したロボットのオブジェクト検索という観点では，まだ研究の余地が大きい．
今後の研究課題として，以下が挙げられる：
(1) シーンテキストとオブジェクトの関連性を捉えたナラティブ表現の開発，
(2) シーンテキストを検索信号として活用するRAGフレームワークの構築，
(3) 実世界のロボットタスクに適した評価ベンチマークの整備．
これらの課題に取り組むことで，より高度なロボットの視覚言語理解が実現できると期待される．
