\section{関連研究}
Vision-languageモデル（VLM）は、ロボティクスのために広く研究されており~\cite{brohan2023saycan, brohan2022rt}、\cite{XIAO2025129963, Ma2024ASO}において包括的にレビューされている。
多様なマルチモーダル検索手法は\cite{Cao+ijcal22, wang2024crossmodalretrievalsystematicreview}にまとめられている。
さらに、シーンテキストの統合は、視覚表現の曖昧性解消において著しく効果的であることが、\cite{Long2018SceneTD, Gupta2022}に記録されている。

\paragraph{ロボティクスのための基盤モデル}
最近の研究は、大規模言語モデル（LLM）やVLM~\cite{brohan2023saycan, Driess2023PaLMEAE, brohan2022rt}などの大規模基盤モデルを活用することで、操作タスクとナビゲーションタスクにわたる顕著な汎化性能を達成できることを示している。
大規模な実ロボット軌跡で訓練されたVision-language-actionモデル~\cite{brohan2022rt,Black2025pi05,wen2025dexvla,team2025gemini,li2025controlvla}は、多様な操作タスクにわたるスケーラブルな方策学習を可能にする。
例えば、Gemini-Robotics \cite{team2025gemini}は、複雑な家庭タスクのために高度な視覚理解とロボット能力を統合している。
しかし、これらのアプローチはシーンテキストを明示的に組み込んでおらず、テキスト情報が重要な意味的グラウンディングを提供するシーンにおいて、マルチモーダル理解が制限されている。

\paragraph{ロボティクスのためのRAG}
いくつかの既存手法~\cite{xie2025embodiedrag, zhu2024raea, xu2024prag, monaci2025rana, wang2025rag6dpose}は、強化されたグラウンディングと計画のために、具現化エージェントにRAGを適用している。
他のアプローチ~\cite{wang2025navrag, fan2024bevinstructor}は、RAGを使用してナビゲーション指示を生成し、それによりvision-languageナビゲーションモデルを改善している。
RAGベースのメモリは、長期的推論のために提案されており~\cite{anwar2025remembr}、他の手法は模倣方策学習のためにデモンストレーションを検索および融合するためにRAGを使用している~\cite{kumar2025collage}。
これらの進歩にもかかわらず、先行研究は検索信号としてシーンテキストを活用しておらず、これは日常環境において視覚的に類似したオブジェクトを曖昧性解消するために不可欠である。


\paragraph{マルチモーダル検索}
多数の研究 \cite{Yenamandra2023HomeRobotOM, robocup}が、自然言語指示に基づくモバイル操作タスクにおける家庭用サービスロボットの性能を評価している。
いくつかの既存手法 \cite{nlmap, Sigurdsson2023RRExBoTRR, dm2rm, relaxformer}は、環境内のターゲットオブジェクトを識別するためにマルチモーダル検索を実行し、自然言語指示に基づいて関連画像のランク付けリストを出力する。
また、マルチモーダル検索を実行してターゲットオブジェクトを見つけ、テキストベースの回答やウェイポイントを出力する手法もある~\cite{xie2025embodiedrag}。
この分野での急速な進歩にもかかわらず、本質的に曖昧性解消情報を含むシーンテキスト（ラベル、標識など）の活用には重要なギャップが残っている。
提案されたSTAREは、シーンテキストを明示的に組み込む新しいマルチモーダルフレームワークを提案することで、この制限に対処し、実世界環境におけるオブジェクト検索精度の向上を可能にする。

\paragraph{シーンテキスト}
シーンテキストを含む画像に関する研究は、テキスト認識 \cite{Zhao2023MultimodalIL, OTE}、テキスト検出 \cite{Liang_2024_CVPR, STEP, Zheng_2024_CVPR}、テキスト検索 \cite{Zeng2024FocusDA, Zheng_2024_CVPR, vista}、視覚的質問応答~\cite{Biten2021LaTrLT, stvqa, Gao2020MultiModalGN}、およびテキスト合成 \cite{Cui_2024_CVPR, Duan2024ODMAT, Santoso2023OnMS}を含む様々なタスクを網羅している。
特に、シーンテキスト検索手法は、OCRで抽出されたテキストと位置情報を視覚特徴と統合して、クエリテキストとのアライメントを改善し、より高い検索精度を達成することが多い。
Mishraら~\cite{ocrvqa}は、シーンテキスト検索タスクを導入し、文字を検出および分類してクエリテキストを含む画像の確率を計算する2段階パイプラインを提案した。
Gomezら~\cite{GomezMaflaECCV2018single}は、テキスト提案を予測し、クエリテキストとの類似性に基づいて画像をランク付けするエンドツーエンドネットワークを提案した。
ViSTA~\cite{vista}は、BERT~\cite{Devlin2019BERTPO}と線形投影をそれぞれ使用してOCRで抽出されたテキストと位置を埋め込み、マルチモーダル統合のためにトランスフォーマー内の融合トークンを介してテキストと位置を視覚特徴と融合する。
これらのアプローチとは対照的に、STAREは、対応するオブジェクトとその属性との関連性を捉えたナラティブ表現にテキストと位置を統合する。
さらに、STAREは、シーンテキストと参照されるオブジェクト間の複雑な関係を効率的にモデル化する補助類似度関数を導入する。

\paragraph{ベンチマーク}
マルチモーダル検索とシーンテキストを含むタスクの分野で、いくつかの標準ベンチマークが提案されている。
前者のうち、COCO \cite{Lin2014MicrosoftCC}とFlickr30K \cite{flickr30k}は、もともと画像キャプショニングベンチマークであり、マルチモーダル検索タスクに広く使用されている。
REVERIE \cite{qi2020reverie}とGOAT-Bench~\cite{khanna2024goatbench}は、オブジェクト目標ナビゲーションタスク用に設計されており、LTRRIE~\cite{multirankit}は画像のランク付けによるマルチモーダル検索に焦点を当てている。
シーンテキストのベンチマークは、様々なvision-languageタスクにわたっている。
例えば、ST-VQA \cite{stvqa}は、シーンテキストベースのVQAに一般的に使用され、TextCaps \cite{textcaps}とCOCO-Text-Captioned \cite{stacmr}は、シーンテキストを考慮した画像キャプショニングに適している。
Visual Genome \cite{Krishna2017}には、シーンテキストと詳細なアノテーションを持つ画像が含まれており、vision-languageグラウンディングと推論タスクをサポートしている。
最後に、RefText \cite{stan}は、参照表現理解のベンチマークであり、画像-オブジェクトペアと参照表現を提供し、その一部はコンテキストの手がかりとしてシーンテキストを活用している。
マルチモーダル検索のための多くの既存ベンチマークの中で、ほとんどはシーンテキストを欠いているため、我々のタスクには適していない。
シーンテキストを含むベンチマークは、通常、マルチモーダル検索用に設計されていないか、ロボットタスクに適していないか、または過度に単純な指示を含んでいる。
これらの制限により、シーンテキストと視覚コンテンツにわたるきめ細かい推論を必要とする、ロボットのための実世界の指示ベースの検索タスクを評価するには不十分である。

\paragraph{既存手法との違い}
STAREは、2つの重要な側面で既存のマルチモーダル検索手法と異なる。
第一に、ほとんどの先行手法が屋内環境での検索に焦点を当てているのに対し、STAREは広範囲の屋内および屋外環境にわたってタスクを実行し、実世界のシナリオにより適用可能である。
第二に、STAREは画像からシーンテキストを抽出および利用するが、既存の手法はシーンテキストを明示的に組み込んでいない。
次に、STAREは、シーンテキストを処理する方法において、シーンテキストを扱うまたはシーンテキストを考慮したマルチモーダル検索を実行する既存の手法と異なる。
シーンテキストをそのまま使用する先行手法とは対照的に、STAREは、対応するオブジェクトとその属性との関連性を捉えたナラティブ表現にシーンテキストを統合する。
また、本手法は、シーンテキストと参照されるオブジェクト間の複雑な関係を効率的にモデル化する補助類似度関数を導入する。
